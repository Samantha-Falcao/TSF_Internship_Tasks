{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Explore Unsupervised Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unsupervised Machine learning** is the ML task of inferring a function to describe hidden structure from 'unlabelled' data where a classification or categorization is not included in the observations. Unsupervised learning is where we only have the input data (X) and no corresponding output variables. One of the commonly used methods to solve an unsupervised learning problem is **Clustering** which inlcudes grouping together unlabelled data points into categories/clusters wherein the data points belonging to a cluster are similar and all the clusters are distinct from each other.\n",
    "\n",
    "One of the simplest and popular unsupervised machine learning algorithms is the **K-Means Clustering Algorithm** which identifies *K* number of centroids (corresponding to *K* clusters of the data set), and then allocates every data point to the nearest cluster, while keeping the centroids as small as possible.\n",
    "\n",
    "In this task, we attempt to explore the unsupervised machine learning using K-Means clustering algorithm.\n",
    "\n",
    "## Task:\n",
    "From the given ‘Iris’ dataset, predict the optimum number of clusters and represent it visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the iris dataset from the scikit-learn library\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# printing the description of the data set\n",
    "print(iris['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imported data set 'iris' is not a dataframe but it is a dictonary. We firstly convert it to a pandas dataframe, without the target column since this is an unsupervised learning problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a pandas DataFrame with the imported iris data\n",
    "df = pd.DataFrame(iris['data'], columns=iris['feature_names'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us obtain some more information about the features of this data set. We create a heatmap on the correlation table of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(), annot=True, cmap='plasma_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the petal length and petal width features have the strongest positive correlation in all three species. And sepal length also is positively correlated to petal length and petal width. However, sepal width has a negative correlation with all other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now look at the clustering technique using K-Means clustering algorithm.\n",
    "\n",
    "**How can we find the optimum number of clusters for K Means? How does one determine the value of K?**\n",
    "\n",
    "The **ELBOW METHOD** is one of the most popular methods to determine the optimal value of K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary function\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []  # wcss corresponds to: within cluster sum of squares\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = k, random_state = 0)\n",
    "    kmeans.fit(df)\n",
    "    wcss.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Intertia is the sum of squared distances of samples to their closest cluster center.\n",
    "\n",
    "We now plot a graph of all the intertia values (within cluster sum of sqaures) to get a visualization of the elbow effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(range(1, 11), wcss, marker='o')\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Within Cluster Sum of Squares') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be clearly seen from the above graph why it is called 'The elbow method'. The idea of this method is to choose the 'K' at which the within cluster sum of squares (WCSS) decreases abruptly and  doesn't decrease significantly with every iteration thereafter. The optimum clusters is where the elbow occurs. \n",
    "\n",
    "From this we choose the optimum  number of clusters as **3**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying kmeans to the dataset and creating the model\n",
    "kmeans = KMeans(n_clusters = 3, random_state = 0)\n",
    "y_kmeans = kmeans.fit_predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the clusters\n",
    "Let us visualize the created clusters and their centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualizing the clusters based on first two columns of the dataset\n",
    "plt.scatter(df['sepal length (cm)'], df['sepal width (cm)'], c=kmeans.labels_)\n",
    "# Plotting the centroids of the clusters\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 100, c = 'red', label = 'Centroids')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the 3 clusters created and the centroids of each. \n",
    "\n",
    "In this task, we have information regarding the target column as well and we know what the classifications are. Hence, we can try to create clusters based on each of these target classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['target_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df[y_kmeans == 0]['sepal length (cm)'], df[y_kmeans == 0]['sepal width (cm)'], \n",
    "            c = 'purple', label = 'Iris-setosa')\n",
    "plt.scatter(df[y_kmeans == 1]['sepal length (cm)'], df[y_kmeans == 1]['sepal width (cm)'], \n",
    "            c = 'teal', label = 'Iris-versicolour')\n",
    "plt.scatter(df[y_kmeans == 2]['sepal length (cm)'], df[y_kmeans == 2]['sepal width (cm)'],\n",
    "         c = 'yellow', label = 'Iris-virginica')\n",
    "\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 100, c = 'red', label = 'Centroids')\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1,1),loc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference in the above two plots is that in the first plot, we dont know which cluster corresponds to which category of the target class (which is the case of unlabelled data) and in the second plot we have this information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
